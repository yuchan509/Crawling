{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 네이버 영화 140자 평 데이터 수집.\n",
    "- 소스보기 했을 때 원하는 데이터가 없다면 iframe 유무 확인.\n",
    "- iframe의 src 속성의 주소를 요청해서 원하는 데이터가 있는지 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요청 함수\n",
    "def getSource(site) :\n",
    "    # 헤더 정보\n",
    "    header_info = {\n",
    "        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # 요청한다.\n",
    "    response = requests.get(site, headers = header_info)\n",
    "    # print(response.text)\n",
    "    \n",
    "    # bs4 객체 생성\n",
    "    soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMovieCode() :\n",
    "    site = 'https://movie.naver.com/movie/running/current.nhn'\n",
    "    soup = getSource( site )\n",
    "   # print(soup)\n",
    "    \n",
    "    # ul 태그를 가져온다. \n",
    "    a1 = soup.select_one('#content > div.article > div:nth-child(1) > div.lst_wrap > ul')\n",
    "   # print(a1)\n",
    "    \n",
    "   # li 태그들을 가져옴.\n",
    "    a2 = a1.select('li')\n",
    "   # print(a2)\n",
    "    \n",
    "    # 코드들을 담을 리스트\n",
    "    code_list = []\n",
    "    \n",
    "    \n",
    "    # li 태그의 수 만큼 반복.\n",
    "    for a3 in a2 :\n",
    "        a4 = a3.select_one('dl > dt > a')\n",
    "        # print(a4)\n",
    "        \n",
    "        # href 속성값을 가져옴.\n",
    "        href = a4.attrs['href']\n",
    "        # print(href)\n",
    "        \n",
    "        # =를 기준으로 문자열을 잘라내여 코드값만 가져옴.\n",
    "        data1 = href.split('=')[1]\n",
    "        # print(data1)\n",
    "        \n",
    "        code_list.append(data1.strip())\n",
    "        \n",
    "    return code_list\n",
    "\n",
    "# k100 = getMovieCode()    \n",
    "# print(k100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 페이지의 데이터를 수집해 저장하는 함수\n",
    "def getData(soup, page) :\n",
    "    \n",
    "    data_dict = {\n",
    "        '평점'      : [],\n",
    "        '네티즌 평' : [],\n",
    "        '작성자'    : [],\n",
    "        '작성시간'  : [],\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # ul 태그를 가져온다.\n",
    "    a1 = soup.select_one('body > div > div > div.score_result > ul')\n",
    "    # print(a1)\n",
    "    \n",
    "    # li 태그들을 가져온다.\n",
    "    a2 = a1.select('li')\n",
    "    # print(a2)\n",
    "    \n",
    "    # li 태그의 수 만큼 반복한다.\n",
    "    for idx, a3 in enumerate(a2) :\n",
    "        # 평점을 가져온다.\n",
    "        a4 = a3.select_one('div.star_score > em')\n",
    "        data1 = a4.text.strip()\n",
    "        # print(data1)\n",
    "\n",
    "        # 후기 글을 가져온다.\n",
    "        a5 = a3.select_one(f'#_filtered_ment_{idx}')\n",
    "        data2 = a5.text.strip()\n",
    "        # print(data2)\n",
    "        \n",
    "        #_filtered_ment_0\n",
    "        #_filtered_ment_5\n",
    "        #_filtered_ment_3\n",
    "        \n",
    "        \n",
    "        # 작성자를 가져온다.\n",
    "        a6 = a3.select_one('div.score_reple > dl > dt > em:nth-child(1) > a > span')\n",
    "        data3 = a6.text.strip()\n",
    "        # print(data3)\n",
    "        \n",
    "        \n",
    "        # 작성 날짜를 가져온다.\n",
    "        a7 = a3.select_one('div.score_reple > dl > dt > em:nth-child(2)')\n",
    "        data4 = a7.text.strip()\n",
    "        # print(data4)\n",
    "        \n",
    "        \n",
    "       # print(data1)\n",
    "       # print(data2)\n",
    "       # print(data3)\n",
    "       # print(data4)\n",
    "        data_dict['평점'].append(data1)\n",
    "        data_dict['네티즌 평'].append(data2)\n",
    "        data_dict['작성자'].append(data3)\n",
    "        data_dict['작성시간'].append(data4)\n",
    "        \n",
    "        # 데이터 프레임 생성\n",
    "        df1 = pd.DataFrame(data_dict)\n",
    "        # display(df1)\n",
    "        \n",
    "        if os.path.exists('data3.csv') == False :\n",
    "            # 파일이 없을 경우\n",
    "            df1.to_csv('data3.csv', encoding='utf-8-sig', index=False)\n",
    "            \n",
    "        else :\n",
    "            df1.to_csv('data3.csv', encoding='utf-8-sig', index=False, header=False, mode='a')\n",
    "            \n",
    "              \n",
    "    # 전체 페이지의 수를 계산한다.\n",
    "    a8 = soup.select_one('body > div > div > div.score_total > strong > em')\n",
    "    data5 = a8.text.strip()\n",
    "    # print(data5)   \n",
    "    # 쉼표 제거.\n",
    "    data5 = data5.replace(',','')  \n",
    "    # 전체페이지 수를 계산.\n",
    "    total_page = int(data5)//10\n",
    "    if int(data5) % 10 > 0 :\n",
    "        total_page +=1\n",
    "    # print(total_page)\n",
    "    \n",
    "    \n",
    "    # 테스트용 코드\n",
    "    total_page = 5\n",
    "    \n",
    "    if total_page > page :\n",
    "        # 전체 페이지 수가 현재 페이지 번호 보다 크면...\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184518 : 5 수집중\n",
      "수집완료\n"
     ]
    }
   ],
   "source": [
    "# 영화 코드들을 가져옴.\n",
    "movie_code_list = getMovieCode()\n",
    "\n",
    "# 테스트용 코드\n",
    "movie_code_list = movie_code_list[:3]\n",
    "\n",
    "\n",
    "for movie_code in movie_code_list :\n",
    "    page = 1\n",
    "\n",
    "    while True :\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "        print( f'{movie_code} : {page} 수집중')\n",
    "        site = f'https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code={movie_code}&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page={page}'\n",
    "        soup = getSource(site)\n",
    "        chk  = getData(soup, page)\n",
    "\n",
    "        if chk == True :\n",
    "            page +=1\n",
    "        else :\n",
    "            break\n",
    "print('수집완료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
